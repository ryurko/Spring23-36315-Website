<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>36-315 Lecture 17</title>
    <meta charset="utf-8" />
    <meta name="author" content="Professor Ron Yurko" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy Code","success":"Copied!","error":"Press Ctrl+C to Copy"})</script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 36-315 Lecture 17
]
.subtitle[
## More Dendrograms and Principal Component Analysis
]
.author[
### Professor Ron Yurko
]
.date[
### 3/27/2023
]

---










## Lecture 16 Demo: [MCU movie data](https://informationisbeautiful.net/visualizations/which-is-the-best-performing-marvel-movie/)

&lt;img src="figs/Lec17/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)

.pull-left[
Start with all observations in their own cluster

- Step 1: Compute the pairwise dissimilarities between each cluster

- Step 2: Identify the pair of clusters that are __least dissimilar__

- Step 3: Fuse these two clusters into a new cluster!

- __Repeat Steps 1 to 3 until all observations are in the same cluster__
]
.pull-right[
&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png" width="85%" style="display: block; margin: auto;" /&gt;

Forms a __dendrogram__ (typically displayed from bottom-up)
]

---

## How do we define dissimilarity between clusters?

We know how to compute distance / dissimilarity between two observations

__But how do we handle clusters?__

  - Dissimilarity between a cluster and an observation, or between two clusters

We need to choose a __linkage function__! Clusters are built up by __linking them together__

Compute all pairwise dissimilarities between observations in cluster 1 with observations in cluster 2

i.e. Compute the distance matrix between observations, `\(d(x_i, x_j)\)` for `\(i \in C_1\)` and `\(j \in C_2\)`


  - __Complete linkage__: Use the __maximum__ value of these dissimilarities: `\(\underset{i \in C_1, j \in C_2}{\text{max}} d(x_i, x_j)\)`
  

  - __Single linkage__: Use the __minimum__ value: `\(\underset{i \in C_1, j \in C_2}{\text{min}} d(x_i, x_j)\)`


  - __Average linkage__: Use the __average__ value: `\(\frac{1}{|C_1| \cdot |C_2|} \sum_{i \in C_1} \sum_{j \in C_2} d(x_i, x_j)\)`
  

Define dissimilarity between two clusters __based on our initial dissimilarity matrix between observations__


---


```r
hc_complete &lt;- hclust(mcu_dist, method = "complete")
plot(hc_complete, ylab = "Pairwise Distance", main = "Complete Linkage", xlab = "MCU Movies")
```

&lt;img src="figs/Lec17/unnamed-chunk-4-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---


```r
hc_single &lt;- hclust(mcu_dist, method = "single")
plot(hc_single, ylab = "Pairwise Distance", main = "Single Linkage", xlab = "MCU Movies")
```

&lt;img src="figs/Lec17/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

#### [`ggdendro` version](https://cran.r-project.org/web/packages/ggdendro/vignettes/ggdendro.html)


```r
library(ggdendro)
*ggdendrogram(hc_complete, theme_dendro = FALSE) +
  labs(y = "Cluster Dissimilarity (based on complete linkage)", 
       title = "Which MCU movies are similar to each other?") + 
  coord_flip() + theme_bw() + theme(axis.title.y = element_blank())
```

&lt;img src="figs/Lec17/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

### Useful to display MDS plot with dendrogram side-by-side

&lt;img src="figs/Lec17/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## How do we assign cluster labels?

#### We cut the dendrogram to return cluster labels

Two ways to specify how to cut the tree using the `cutree` function:

1) via the height using `h`, e.g., cut the tree at height = 10


```r
cutree(hc_complete, h = 10)
```


2) via the desired number of clusters `k` - and let the computer figure out the height for us, e.g., `k = 2`


```r
cutree(hc_complete, k = 2)
```


---


```r
mcu_clusters &lt;- cutree(hc_complete, h = 10)
mcu_clusters
```

```
##                           Ant-Man                Ant-Man &amp; The Wasp 
##                                 1                                 1 
##           Avengers: Age of Ultron                Avengers: End Game 
##                                 1                                 2 
##            Avengers: Infinity War                     Black Panther 
##                                 2                                 2 
##                   Black Panther 2                       Black Widow 
##                                 1                                 1 
##                   Captain America        Captain America: Civil War 
##                                 1                                 1 
##   Captain America: Winter Soldier                    Captain Marvel 
##                                 1                                 2 
##                        Dr Strange Dr Strange: Multiverse of Madness 
##                                 1                                 1 
##                          Eternals           Guardians of the Galaxy 
##                                 1                                 1 
##         Guardians of the Galaxy 2                   Incredible Hulk 
##                                 1                                 1 
##                          Iron Man                        Iron Man 2 
##                                 1                                 1 
##                        Iron Man 3                         Shang-Chi 
##                                 1                                 1 
##         Spider-Man: Far from Home            Spider-Man: Homecoming 
##                                 1                                 1 
##           Spider-Man: No Way Home                      The Avengers 
##                                 2                                 2 
##                  Thor: Dark World              Thor: Love &amp; Thunder 
##                                 1                                 1 
##                    Thor: Ragnarok                              Thor 
##                                 1                                 1
```


---

### View results with cut on dendrogram

&lt;img src="figs/Lec17/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

### [`factoextra` package](https://rpkgs.datanovia.com/factoextra/index.html) version


```r
library(factoextra)
fviz_dend(hc_complete, cex = 0.5, k = 3, color_labels_by_k = TRUE)
```

&lt;img src="figs/Lec17/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## Main Takeaways

#### Dendrograms are a great way to visualize distances and the clustering structure in the dataset

#### However there are several decisions to be made! 

#### What type of linkage is appropriate for the problem? 

#### How do we [choose the number of clusters](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/)? 

#### There is NOT a one size fits all solution to any of this!



---

## Conceptual review

#### We've been considering visuals for high-dimensional data

Common workflow:

+ Reduce the data to a few "useful" dimensions

+ Plot those "useful" dimensions


Last two classes:

1. Reduce the data by summarizing pairs of subjects with one distance.

2. Visualize distances using multi-dimensional scaling or dendrograms.

How can we reduce the data without distances?

#### Principal Component Analysis (PCA) is by far the most popular way


---

## Dimension reduction - searching for variance

__GOAL__: Focus on reducing dimensionality of feature space, i.e., number of columns, while __retaining__ most of the information, i.e., __variance__, in a lower dimensional space

- `\(n \times p\)` matrix `\(\rightarrow\)` dimension reduction technique `\(\rightarrow\)` `\(n \times k\)` matrix

--

Special case we just discussed: __MDS__

- `\(n \times n\)` __distance__ matrix `\(\rightarrow\)` MDS `\(\rightarrow\)` `\(n \times k\)` matrix (usually `\(k = 2\)`)

- This requires converting data into a distance matrix - summarizing all differences between observations into a single number, effectively "double reduction"

1. Reduce data to a distance matrix

2. Reduce distance matrix to `\(k = 2\)` dimensions

#### How can we apply dimension to the original data?

---

## Principal Component Analysis (PCA)

$$
`\begin{pmatrix}
&amp; &amp; \text{really} &amp; &amp; \\
&amp; &amp; \text{wide} &amp; &amp; \\
&amp; &amp; \text{matrix} &amp; &amp;
\end{pmatrix}`
\rightarrow \text{matrix algebra stuff} \rightarrow 
`\begin{pmatrix}
\text{much}  \\
\text{thinner}  \\
\text{matrix} 
\end{pmatrix}`
$$

- Start with `\(n \times p\)` matrix of __correlated__ variables `\(\rightarrow\)` `\(n \times k\)` matrix of __uncorrelated__ variables

--

- Each of the `\(k\)` columns in the right-hand matrix are __principal components__, all uncorrelated with each other

- First column accounts for most variation in the data, second column for second-most variation, and so on

#### Intuition: first few principal components account for most of the variation in the data

---

## What are principal components?

- Assume `\(\boldsymbol{X}\)` is a `\(n \times p\)` matrix that is __centered__ and __stardardized__

- _Total variation_ `\(= p\)`, since Var( `\(\boldsymbol{x}_j\)` ) = 1 for all `\(j = 1, \dots, p\)`

- PCA will give us `\(p\)` principal components that are `\(n\)`-length columns - call these `\(Z_1, \dots, Z_p\)`

--

__First principal component__ (aka PC1):

`$$Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1} X_p$$`


--

  - `\(\phi_{j1}\)` are the weights indicating the contributions of each variable `\(j \in 1, \dots, p\)`
  
  - Weights are normalized `\(\sum_{j=1}^p \phi_{j1}^2 = 1\)`
  
  - `\(\phi_{1} = (\phi_{11}, \phi_{21}, \dots, \phi_{p1})\)` is the __loading vector__ for PC1

--
  
  - `\(Z_1\)` is a linear combination of the `\(p\)` variables that has the __largest variance__

---

## What are principal components?

__Second principal component__:

`$$Z_2 = \phi_{12} X_1 + \phi_{22} X_2 + \dots + \phi_{p2} X_p$$`

  - `\(\phi_{j2}\)` are the weights indicating the contributions of each variable `\(j \in 1, \dots, p\)`
  
  - Weights are normalized `\(\sum_{j=1}^p \phi_{j1}^2 = 1\)`
  
  - `\(\phi_{2} = (\phi_{12}, \phi_{22}, \dots, \phi_{p2})\)` is the __loading vector__ for PC2
  
  - `\(Z_2\)` is a linear combination of the `\(p\)` variables that has the __largest variance__
  
    - __Subject to constraint it is uncorrelated with `\(Z_1\)`__ 

--

We repeat this process to create `\(p\)` principal components

- __Uncorrelated__: Each ( `\(Z_j, Z_{j'}\)` ) is uncorrelated with each other

- __Ordered Variance__: Var( `\(Z_1\)` ) `\(&gt;\)` Var( `\(Z_2\)` ) `\(&gt; \dots &gt;\)` Var( `\(Z_p\)` )

- __Total Variance__: `\(\sum_{j=1}^p \text{Var}(Z_j) = p\)`


#### Intuition: pick some `\(k &lt;&lt; p\)` such that if `\(\sum_{j=1}^k \text{Var}(Z_j) \approx p\)`, then just using `\(Z_1, \dots, Z_k\)`

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

&lt;img src="figs/Lec17/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

&lt;img src="figs/Lec17/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

&lt;img src="figs/Lec17/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

&lt;img src="figs/Lec17/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

&lt;img src="figs/Lec17/unnamed-chunk-17-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## So what do we do with the principal components?

__The point__: given a dataset with `\(p\)` variables, we can find `\(k\)` variables `\((k &lt;&lt; p)\)` that account for most of the variation in the data

--

Note that the principal components are NOT easy to interpret - these are combinations of all variables

PCA is similar to MDS with these main differences:

1. MDS reduces a _distance_ matrix while PCA reduces a _data_ matrix

2. PCA has a principled way to choose `\(k\)`

3. Can visualize how the principal components are related to variables in data

---

## Working with PCA on Starbucks drinks

Use the `prcomp()` function (based on SVD) for PCA on __centered__ and __scaled__ data


```r
*starbucks_pca &lt;- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg),
*                       center = TRUE, scale. = TRUE)
summary(starbucks_pca)
```

```
## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6    PC7
## Standard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413
## Proportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177
## Cumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894
##                            PC8     PC9    PC10    PC11
## Standard deviation     0.28123 0.16874 0.08702 0.04048
## Proportion of Variance 0.00719 0.00259 0.00069 0.00015
## Cumulative Proportion  0.99657 0.99916 0.99985 1.00000
```

---

## Computing Principal Components

Extract the matrix of principal components `\(\boldsymbol{Z} = XV\)` (dimension of `\(\boldsymbol{Z}\)` will match original data)


```r
starbucks_pc_matrix &lt;- starbucks_pca$x
head(starbucks_pc_matrix)
```

```
##            PC1        PC2        PC3           PC4         PC5         PC6
## [1,] -3.766852 -1.0023657  0.2482698 -0.1521871448  0.24739830 -0.11365847
## [2,] -3.633234 -0.6946439  1.2059943 -0.3720566566  0.06052789 -0.06406410
## [3,] -3.518063 -0.3981399  2.2165170 -0.5967175941 -0.13122572 -0.01937237
## [4,] -3.412061 -0.1067045  3.3741594 -0.8490378243 -0.26095965 -0.00899485
## [5,] -3.721426 -0.9868147 -1.0705094  0.0949330091 -0.27181508  0.17491809
## [6,] -3.564899 -0.6712499 -0.7779083 -0.0003019903 -0.72054963  0.37005543
##              PC7         PC8        PC9        PC10         PC11
## [1,] -0.02812472 0.006489978 0.05145094 -0.06678083 -0.019741873
## [2,]  0.05460952 0.021148978 0.07094211 -0.08080545 -0.023480029
## [3,]  0.09050806 0.031575955 0.08901403 -0.09389227 -0.028669251
## [4,]  0.11585507 0.037521689 0.11287190 -0.11582260 -0.034691142
## [5,]  0.07009414 0.037736197 0.02892317 -0.03631676 -0.005775410
## [6,]  0.20236484 0.068154160 0.03705252 -0.03497690 -0.002469611
```

Columns are uncorrelated, such that Var( `\(Z_1\)` ) `\(&gt;\)` Var( `\(Z_2\)` ) `\(&gt; \dots &gt;\)` Var( `\(Z_p\)` ) - can start with a scatterplot of `\(Z_1, Z_2\)`

---

## Starbucks drinks: PC1 and PC2

.pull-left[

```r
starbucks &lt;- starbucks %&gt;%
  mutate(pc1 = starbucks_pc_matrix[,1], 
         pc2 = starbucks_pc_matrix[,2])
starbucks %&gt;%
  ggplot(aes(x = pc1, y = pc2)) +
  geom_point(alpha = 0.5) +
  labs(x = "PC 1", y = "PC 2")
```

- __Look familiar?__

- Principal components are not interpretable, but we can add a __biplot__ with arrows showing the linear relationship between one variable and other variables

]

.pull-right[
&lt;img src="figs/Lec17/unnamed-chunk-20-1.png" width="100%" /&gt;

]

---

## Making PCs interpretable with biplots ([`factoextra`](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization))

.pull-left[



```r
library(factoextra)
# Designate to only label the variables:
*fviz_pca_biplot(
* starbucks_pca, label = "var",
  # Change the alpha for observations 
  # which is represented by ind
  alpha.ind = .25,
  # Modify the alpha for variables (var):
  alpha.var = .75,
  col.var = "darkblue")
```

- Arrow direction: "as the variable increases..."

- Arrow angles: correlation

  - 90 degrees means uncorrelated
  - `\(&lt; 90\)` means positively correlated
  - `\(&gt; 90\)` means negatively correlated
  
- Arrow length: strength of relationship with PCs

]
.pull-right[
&lt;img src="figs/Lec17/unnamed-chunk-21-1.png" width="100%" /&gt;

]

---

## How many principal components to use?

#### Intuition: Additional principal components will add smaller and smaller variance

- Keep adding components until the added variance _drops off_


```r
summary(starbucks_pca)
```

```
## Importance of components:
##                           PC1    PC2    PC3     PC4     PC5     PC6    PC7
## Standard deviation     2.4748 1.3074 1.0571 0.97919 0.67836 0.56399 0.4413
## Proportion of Variance 0.5568 0.1554 0.1016 0.08716 0.04183 0.02892 0.0177
## Cumulative Proportion  0.5568 0.7122 0.8138 0.90093 0.94276 0.97168 0.9894
##                            PC8     PC9    PC10    PC11
## Standard deviation     0.28123 0.16874 0.08702 0.04048
## Proportion of Variance 0.00719 0.00259 0.00069 0.00015
## Cumulative Proportion  0.99657 0.99916 0.99985 1.00000
```

---

## Create scree plot (aka "elbow plot") to choose


```r
*fviz_eig(starbucks_pca, addlabels = TRUE) +
  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = "dashed", color = "darkred")
```

&lt;img src="figs/Lec17/scree-plot-1.png" width="80%" style="display: block; margin: auto;" /&gt;

- Number of dimensions on x-axis, proportion of variance on y-axis

- _Rule of thumb_: horizontal line at `\(1/p\)` (__Why?__)

---

## Main Takeaways

#### It's really hard to visualize many dimensions at the same time

- Often, it makes a lot of sense to choose 2-4 of the "most important dimensions" and just plot those

#### PCA is a very common way to define "most important dimensions"

#### PCA provides the linear combinations of variables that capture the most variation in the data.

#### Common to plot the first two principal components in a scatterplot, and see if subjects cluster based on other variables (similar to MDS)

#### Especially useful to plot principal components with a biplot (e.g., with `factoextra`)

- Adds interpretability to the principal components, and helps you see relationships among the variables

--

+ __HW6 is Wednesday March 29th!__

+ __Graphics critique due April 7th!__

+ Review more code in Lecture 17 Demo! 


#### Next time: Spatial data

---

## BONUS: [__Singular Value Decomposition (SVD)__](https://en.wikipedia.org/wiki/Singular_value_decomposition)

$$
X = U D V^T
$$

- Matrices `\(U\)` and `\(V\)` contain the left and right (respectively) __singular vectors of scaled matrix `\(X\)`__

- `\(D\)` is the diagonal matrix of the __singular values__

- SVD simplifies matrix-vector multiplication as __rotate, scale, and rotate again__


`\(V\)` is called the __loading matrix__ for `\(X\)` with `\(\phi_{j}\)` as columns, 

  - `\(Z = X  V\)` is the PC matrix

---

### BONUS: how to solve time-travel!

#### Eigenvalue decomposition (aka spectral decomposition)

- `\(V\)` are the __eigenvectors__ of `\(X^TX\)` (covariance matrix, `\(^T\)` means _transpose_)

- `\(U\)` are the __eigenvectors__ of `\(XX^T\)`

- The singular values (diagonal of `\(D\)`) are square roots of the __eigenvalues__ of `\(X^TX\)` or `\(XX^T\)`

- Meaning that `\(Z = UD\)`

- `prcomp` function in `R` uses SVD for PCA

&lt;img src="https://thumbs.gfycat.com/RealisticFragrantHerculesbeetle-size_restricted.gif" width="45%" style="display: block; margin: auto;" /&gt;


---

## BONUS: Eigenvalues guide dimension reduction

We want to choose `\(p^* &lt; p\)` such that we are explaining variation in the data

Eigenvalues `\(\lambda_j\)` for `\(j \in 1, \dots, p\)` indicate __the variance explained by each component__

  - `\(\sum_j^p \lambda_j = p\)`, meaning `\(\lambda_j \geq 1\)` indicates `\(\text{PC}j\)` contains at least one variable's worth in variability
  
  - `\(\lambda_j / p\)` equals proportion of variance explained by `\(\text{PC}j\)`
  
  - Arranged in descending order so that `\(\lambda_1\)` is largest eigenvalue and corresponds to PC1
  
  - Can compute the cumulative proportion of variance explained (CVE) with `\(p^*\)` components:
  
`$$\text{CVE}_{p^*} = \frac{\sum_j^{p*} \lambda_j}{p}$$`

Can use [__scree plot__](https://en.wikipedia.org/wiki/Scree_plot) to plot eigenvalues and guide choice for `\(p^* &lt;p\)` by looking for "elbow" (rapid to slow change)


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
