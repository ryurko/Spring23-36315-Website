---
title: "36-315 Lecture 3"
subtitle: "Statistical Inference for 1D Categorical Data"  
author: 
  - "Professor Ron Yurko"
date: '1/25/2023'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  fig.path = "figs/Lec3/"
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#c41230",
  secondary_color = "#0277BD",
  inverse_header_color = "#FFFFFF"
)
```



## What does a bar chart show?

#### Marginal Distribution

- Assume categorical variable $X$ has $K$ categories: $C_1, \dots, C_K$

- __True__ marginal distribution of $X$: 

$$
P(X = C_j) = p_j,\ j \in \{ 1, \dots, K \}
$$

--

#### We have access to the Empirical Marginal Distribution

```{r, include = FALSE}
library(tidyverse)
library(palmerpenguins)
```


- Observed distribution of $X$, our best estimate (MLE) of the marginal distribution of $X$: $\hat{p}_1$, $\hat{p}_2$, $\dots$, $\hat{p}_K$


```{r}
# Proportion estimates for penguins species
table(penguins$species) / nrow(penguins)
```


---

## Bar charts with proportions

.pull-left[

- [`after_stat()`](https://ggplot2.tidyverse.org/reference/aes_eval.html) indicates the aesthetic mapping is performed after statistical transformation

- Use `after_stat(count)` to access the `stat_count()` called by `geom_bar()`

```{r bar-prop-chart, fig.show = 'hide'}
penguins %>% 
  ggplot(aes(x = species)) +
  geom_bar(aes(y = after_stat(count) / #<<
                 sum(after_stat(count)))) + #<<
  labs(y = "Proportion")
```

- Kind of weird code to use...

]

.pull-right[

```{r ref.label = 'bar-prop-chart', echo = FALSE, fig.width=5, fig.height=4}
```

]

---

## Compute and display the proportions directly


.pull-left[

```{r bar-prop-chart-group, fig.show = 'hide'}
penguins %>%
  group_by(species) %>% #<<
  summarize(count = n(), #<<
            .groups = "drop") %>% #<<
  mutate(total = sum(count), #<<
         prop = count / total) %>% #<<
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop),
           stat = "identity") 
```

- Use `group_by()`, `summarize()`, and `mutate()` in a pipeline to compute then display the proportions directly

- Need to indicate we are displaying the `y` axis as given, i.e., the identity function

]

.pull-right[

```{r ref.label = 'bar-prop-chart-group', echo = FALSE, fig.width=5, fig.height=4}
```

]

---

## Statistical inference for proportions

- Our estimate for $p_j$ is $\hat{p}_j = \frac{n_j}{n}$, compute the standard error as:

$$
SE(\hat{p}_j) = \sqrt{\frac{\hat{p}_j(1 - \hat{p}_j)}{n}}
$$

--

- Compute $\alpha$-level __confidence interval__ (CI) as $\hat{p}_j \pm z_{1 - \alpha / 2} \cdot SE(\hat{p}_j)$

  + In code: $z_{1 - \alpha / 2}$ = `qnorm(1 - alpha / 2)`

- Good rule-of-thumb: construct 95% CI using $\hat{p}_j \pm 2 \cdot SE(\hat{p}_j)$

--

- Uses a Normal approximation justified by CLT (so CI could include values outside of [0,1]...):

$$
\sqrt{n} (\hat{p}_j - p_j) \rightarrow \text{Normal}(0,\ p_j(1 - p_j))
$$
- To get CI for counts: just multiply $\hat{p}$ by $n$



---

## Add standard errors to bars

.pull-left[

```{r bar-prop-chart-se, fig.show = 'hide'}
penguins %>%
  group_by(species) %>% 
  summarize(count = n(), .groups = "drop") %>% 
  mutate(total = sum(count), 
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total), #<<
         lower = prop - 2 * se, #<<
         upper = prop + 2 * se) %>% #<<
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower, #<<
                    ymax = upper), #<<
                color = "red") #<<
```


- If CIs don‚Äôt overlap $\rightarrow$ likely significant difference

- If CIs overlap a little $\rightarrow$ ambiguous

- If CIs overlap a lot $\rightarrow$ no significant difference

]

.pull-right[

```{r ref.label = 'bar-prop-chart-se', echo = FALSE, fig.width=5, fig.height=4}
```

]

---

## Don't do this...

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Wait a second, these are not real error bars ‚Ä¶ the author literally just put the letter ‚ÄúT‚Äù above the bar graphs üò≠ <a href="https://t.co/KKtTGRHFaw">pic.twitter.com/KKtTGRHFaw</a></p>&mdash; Josemari Feliciano (@SeriFeliciano) <a href="https://twitter.com/SeriFeliciano/status/1597355324008108034?ref_src=twsrc%5Etfw">November 28, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---

## Back up a bit: Chi-Squared Test

- The chi-squared test is the main, high-level test for categorical data

- Determines if there's a significant difference between the __expected__ and __observed__ frequencies across categories _at a global level_

--

- 2D Categorical Data: Tests independent of two categorical variables (more on this next week)

- 1D Categorical Data: Test if proportions across categories are equal:

$$
\begin{align}
\text{Null: } H_0:& \ p_1 = p_2 = \cdot \cdot \cdot = p_K = \frac{1}{K}\\
\text{Alternative: } H_A:& \text{ The probabilities are not all equal}
\end{align}
$$

- note the alternative is __NOT__ the same as $p_1 \neq = p_2 \neq \cdot \cdot \cdot \neq p_K$

---

## Computing and Interpreting the Chi-Square Test

$$
\text{Test Statistic: }\chi^2 = \sum_{j = 1}^K \frac{(O_j - E_j)^2}{E_j}
$$

- $O_j$: Observed counts in category $C_j$

- $E_j$: Expected counts under $H_0$, i.e., each category is equally likely to occur $n/K = p_1 = \cdot \cdot \cdot = p_K$)

--

.pull-left[
```{r}
chisq.test(table(penguins$species)) #<<
```

]
.pull-right[

#### Interpretation

<!-- - Large $\chi^2 \rightarrow$ observed counts are very different from expected counts -->

<!-- - Therefore we should just reject the null hypothesis -->

<!-- - i.e., the $p$-value is small because we would not expect to see observed counts so extreme if the null were true -->

<!-- - But if we reject, cannot tell _which_ probabilities are different... -->

<!-- - _Can we use graphs to tell us which are different...?_ -->
]

---

# Hypothesis testing review

.pull-left[
Computing $p$-values works like this:

- Choose a test statistic.

- Compute the test statistic in your dataset.

- Is test statistic "unusual" compared to what I would expect under $H_0$?

- Compare $p$-value to __target error rate__ $\alpha$ (typically referred to as target level $\alpha$ )

- Typically choose $\alpha = 0.05$ 

  - i.e., if we reject null  hypothesis at $\alpha = 0.05$ then, assuming $H_0$ is true, there is a 5% chance it is a false positive (aka Type 1 error)
]

--


.pull-right[

```{r, echo = FALSE, out.width = '100%', fig.align='center'}
knitr::include_graphics("https://measuringu.com/wp-content/uploads/2021/04/042121-F2.jpg") 
```


]


---

## Computing and Interpreting the Chi-Square Test

$$
\text{Test Statistic: }\chi^2 = \sum_{j = 1}^K \frac{(O_j - E_j)^2}{E_j}
$$

- $O_j$: Observed counts in category $C_j$

- $E_j$: Expected counts under $H_0$, i.e., each category is equally likely to occur $n/K = p_1 = \cdot \cdot \cdot = p_K$)

--

.pull-left[
```{r}
chisq.test(table(penguins$species)) #<<
```

]
.pull-right[

- However, if we reject, we cannot tell _which_ probabilities are different...

- _Can we use graphs to tell us which are different...?_

]


---

## Graphics versus Statistical Inference

- Reminder - Anscombe's Quartet: where statistical inference was the same but the graphics were very different

```{r, echo = FALSE, out.width="55%", fig.align='center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/1200px-Anscombe%27s_quartet_3.svg.png")
```


--

- __The opposite can be true!__ Graphics are the same, but statistical inference is very different...


---

### Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r, echo = FALSE, fig.align='center'}
fake_counts <- tibble(fake_group = c("A", "A", "B", "C"))
fake_counts %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

### Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r low-count-noted-bars, echo = FALSE, fig.align='center'}
# Init the fake data
fake_counts <- tibble(fake_group = c("A", "A", "B", "C"))
# Run and store the chi-square test:
fake_chisq_test_results <- chisq.test(table(fake_counts$fake_group))
# Create the plot
fake_counts %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = 1.5, size = 5,
           label = paste0("n = ", nrow(fake_counts))) +
  # Add label with p-value from chi-square test
  annotate(geom = "text", x = 2.5, y = 1.25, size = 5,
           label = paste0("p-value = ", 
                          signif(fake_chisq_test_results$p.value,
                                 digits = 2))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

### Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r mid-count-noted-bars, echo = FALSE, fig.align='center'}
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 8), 
                                     rep("B", 4), rep("C", 4)))
# Run and store the chi-square test:
fake_chisq_test_results <- chisq.test(table(fake_counts$fake_group))
fake_counts %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  # Add labels but preserve location of vertical label based on change in n
  annotate(geom = "text", x = 2.5, y = 1.5 * 8 / 2, size = 5,
           label = paste0("n = ", nrow(fake_counts))) +
  annotate(geom = "text", x = 2.5, y = 1.25 * 8 / 2, size = 5,
           label = paste0("p-value = ", 
                          signif(fake_chisq_test_results$p.value,
                                 digits = 2))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

### Example: 3 categories, $p_1 = 1/2,\ p_2 = p_3 = 1/4$

```{r high-count-noted-bars, echo = FALSE, fig.align='center'}
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))
# Run and store the chi-square test:
fake_chisq_test_results <- chisq.test(table(fake_counts$fake_group))
# Create the plot
fake_counts %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue") +
  # Add labels but preserve location of vertical label based on change in n
  annotate(geom = "text", x = 2.5, y = 1.5 * 32 / 2, size = 5,
           label = paste0("n = ", nrow(fake_counts))) +
  annotate(geom = "text", x = 2.5, y = 1.25 * 32 / 2, size = 5,
           label = paste0("p-value = ", 
                          signif(fake_chisq_test_results$p.value,
                                 digits = 2))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```


---

## Power under this scenario: (2n/4, n/4, n/4)

```{r chi-power-curve, echo = FALSE, fig.align='center'}
# Iterate through a sequence of sample sizes to compute the pvalue:
chisq_power_table <-
  map_dfr(seq(4, 100, by = 4),
          # Apply the following function to each value
          function(x) {
            
            # Init the fake data:
            fake_data <- tibble(fake_group = 
                                  c(rep("A", 2 * x / 4), 
                                    rep("B", x / 4), rep("C", x / 4)))
            
            # Run and store the chi-square test:
            fake_results <- chisq.test(table(fake_data$fake_group))
            
            # Return a table with the p-value and n:
            tibble("n" = x,
                   "pval" = fake_results$p.value)
          })

# Plot the results:
chisq_power_table %>%
  # Note the use of 
  ggplot(aes(x = n, y = pval)) +
  # Draw a line layer to connect the points 
  geom_line(color = "gray") +
  geom_point(color = "black") +
  # Add a horizontal line at 0.05:
  geom_hline(yintercept = 0.05, linetype = "dashed", 
             color = "red") +
  scale_x_continuous(breaks = seq(0, 100, by = 20)) +
  theme_bw() +
  labs(x = "Sample size", y = "p-value")

```


---

## How do we combine graphs with inference?

1. Simply add $p$-values (or other info) to graph via text

2. Add confidence intervals to the graph

  - Need to remember what each CI is for! 
  
  - Our CIs on previous slides are for each $\hat{p}_j$ marginally, __NOT__ jointly

  - Have to be careful with __multiple testing__...

---

## CIs will visually capture uncertainty in estimates

.pull-left[

```{r low-count-bars-se, echo = FALSE, fig.align='center', fig.height=7}
# Init the fake data
fake_counts <- tibble(fake_group = c("A", "A", "B", "C"))
# Compute the summary with standard errors:
fake_counts %>%
  group_by(fake_group) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - 2 * se,
         upper = prop + 2 * se) %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```

]
.pull-right[

```{r high-count-bars-se, echo = FALSE, fig.align='center', fig.height = 7}
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))# Compute the summary with standard errors:
fake_counts %>%
  group_by(fake_group) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - 2 * se,
         upper = prop + 2 * se) %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```

]

---

## (Rough) Rules-of-thumb for comparing CIs on bar charts

- Comparing overlap of two CIs is __NOT__ exactly the same as directly testing for a significant difference...

  - Really you want CI( $\hat{p}_1 - \hat{p}_2$ ), not CI( $\hat{p_1}$ ) and CI( $\hat{p_2} $)
  
  - CI( $\hat{p_1}$ ) and CI( $\hat{p_2}$ ) not overlapping implies $0 \notin$ CI( $\hat{p}_1 - \hat{p}_2$ )
  
--

  - _However_ CI( $\hat{p_1}$ ) and CI( $\hat{p_2}$ ) overlaping __DOES NOT__ imply $0 \in$ CI( $\hat{p}_1 - \hat{p}_2$ ) 


--

Roughly speaking:

  - If CIs don't overlap $\rightarrow$ significant difference
  
  - If CIs overlap a little $\rightarrow$ ambiguous
  
  - If CIs overlap a lot $\rightarrow$ no significant difference
  
--

But if we're comparing more than two CIs simultaneously, we need to account for __multiple testing__!

  - When you look for all non-overlapping CIs: implicitly making $\binom{K}{2} = \frac{K!}{2!(K-2)!}$ pairwise tests in your head!
  

---

## Corrections for multiple testing (and visualizing them)

- In those bar plots, when we determine whether CIs overlap we make 3 comparisons:

  1. A vs B
  
  2. A vs C
  
  3. B vs C
  
#### This is a multiple testing issue

--

- In short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!

- Reminder: Type 1 error = Rejecting $H_0$ when $H_0$ is true

- e.g., CIs don't overlap but actually $H_0: p_A = p_B$ is true
  
- If only interested in A vs B __and nothing else__, then just construct 95% CI for A vs B and _control error rate_ at 5%
  
- However, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate > 5%!

---

## Corrections for multiple testing (and visualizing them)

Vast literature on corrections for multiple testing (beyond the scope of this class... but in my thesis!)

- But you should understand the following:

1. Corrections for multiple testing inflate $p$-values (i.e., make them bigger)

2. Equivalently, they inflate CIs (i.e., make them wider)

3. Purpose of these corrections is to control Type 1 error rate $\leq 5\%$

--

We'll focus on the __Bonferroni correction__, which inflates $p$-values the most but is easy to implement and very popular:

+ We usually reject null hypothesis when $p$-value $\leq .05$

+ __Bonferroni__: if making $K$ comparisons, reject only if $p$-value $\leq .05/K$

--

+ For CIs: instead of plotting 95% CIs, we plot (1 - $0.05/K$)% CIs

  + e.g., for $K = 3$ then plot 98.3% CIs
  
---

## Impact of Bonferroni correction on CIs...

.pull-left[
```{r ref.label="high-count-bars-se", eval = TRUE, echo = FALSE, fig.align='center', fig.height = 7}

```
]
.pull-right[

```{r larger-intervals, echo = FALSE, fig.align='center', fig.height = 7}
# Init the fake data
fake_counts <- tibble(fake_group = c(rep("A", 32), 
                                     rep("B", 16), rep("C", 16)))# Compute the summary with standard errors:
fake_counts %>%
  group_by(fake_group) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(total = sum(count),
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total),
         lower = prop - qnorm(1 - .05 / 3 / 2) * se,
         upper = prop + qnorm(1 - .05 / 3 / 2) * se) %>%
  ggplot(aes(x = fake_group)) +
  geom_bar(fill = "darkblue",
           aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower,
                    ymax = upper),
                color = "red") +
  # Add the label with number of observations
  annotate(geom = "text", x = 2.5, y = .75, size = 10,
           label = paste0("n = ", nrow(fake_counts))) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_bw() +
  theme(axis.title = element_blank(),
        axis.text = element_text(size = 16))

```

]

---

## Main Takeaways

+ Graphs looking at 1D categorical data (e.g., bar charts) look at the empirical distribution of the categorical variable ( $\hat{p}_1, \dots, \hat{p}_K$ )

+ Chi-squared test is the main statistical test for 1D categorical data (_global test_), testing $H_0 : p_1 = \cdot \cdot \cdot p_K$

+ However, from this test alone, we can‚Äôt tell which probabilities differ

+  Can generate individual CIs for each $\hat{p}_1$, $\dots$, $\hat{p}_K$

  +  Allows for easy visualization.
  
  + But can be complicated, especially with respect to multiple testing.
  
+ Graphs with the same trends can display very different statistical significance (largely due to sample size)

  
---

## Useful to order categories by frequency with [`forcats`](https://forcats.tidyverse.org/)

.pull-left[

```{r bar-prop-chart-order, fig.show = 'hide'}
penguins %>%
  group_by(species) %>% 
  summarize(count = n(), .groups = "drop") %>% 
  mutate(total = sum(count), 
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total), 
         lower = prop - 2 * se, 
         upper = prop + 2 * se,
         species = 
           fct_reorder(species, prop)) %>% #<<
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower, 
                    ymax = upper), 
                color = "red") 
```


]

.pull-right[

```{r ref.label = 'bar-prop-chart-order', echo = FALSE, fig.width=5, fig.height=4}
```

]



---
class: center, middle

# Next time: 2D categorical data

### HW1 is posted and due Wednesday Feb 1 by 11:59 PM

Recommended reading: 

[CW Chapter 16.2 Visualizing the uncertainty of point estimates](https://clauswilke.com/dataviz/visualizing-uncertainty.html#visualizing-the-uncertainty-of-point-estimates)





