<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>36-315 Lecture 3</title>
    <meta charset="utf-8" />
    <meta name="author" content="Professor Ron Yurko" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 36-315 Lecture 3
]
.subtitle[
## Statistical Inference for 1D Categorical Data
]
.author[
### Professor Ron Yurko
]
.date[
### 1/25/2023
]

---








## What does a bar chart show?

#### Marginal Distribution

- Assume categorical variable `\(X\)` has `\(K\)` categories: `\(C_1, \dots, C_K\)`

- __True__ marginal distribution of `\(X\)`: 

$$
P(X = C_j) = p_j,\ j \in \{ 1, \dots, K \}
$$

--

#### We have access to the Empirical Marginal Distribution




- Observed distribution of `\(X\)`, our best estimate (MLE) of the marginal distribution of `\(X\)`: `\(\hat{p}_1\)`, `\(\hat{p}_2\)`, `\(\dots\)`, `\(\hat{p}_K\)`



```r
# Proportion estimates for penguins species
table(penguins$species) / nrow(penguins)
```

```
## 
##    Adelie Chinstrap    Gentoo 
## 0.4418605 0.1976744 0.3604651
```


---

## Bar charts with proportions

.pull-left[

- [`after_stat()`](https://ggplot2.tidyverse.org/reference/aes_eval.html) indicates the aesthetic mapping is performed after statistical transformation

- Use `after_stat(count)` to access the `stat_count()` called by `geom_bar()`


```r
penguins %&gt;% 
  ggplot(aes(x = species)) +
* geom_bar(aes(y = after_stat(count) /
*                sum(after_stat(count)))) +
  labs(y = "Proportion")
```

- Kind of weird code to use...

]

.pull-right[

&lt;img src="figs/Lec3/unnamed-chunk-3-1.png" width="100%" /&gt;

]

---

## Compute and display the proportions directly


.pull-left[


```r
penguins %&gt;%
* group_by(species) %&gt;%
* summarize(count = n(),
*           .groups = "drop") %&gt;%
* mutate(total = sum(count),
*        prop = count / total) %&gt;%
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop),
           stat = "identity") 
```

- Use `group_by()`, `summarize()`, and `mutate()` in a pipeline to compute then display the proportions directly

- Need to indicate we are displaying the `y` axis as given, i.e., the identity function

]

.pull-right[

&lt;img src="figs/Lec3/unnamed-chunk-4-1.png" width="100%" /&gt;

]

---

## Statistical inference for proportions

- Our estimate for `\(p_j\)` is `\(\hat{p}_j = \frac{n_j}{n}\)`, compute the standard error as:

$$
SE(\hat{p}_j) = \sqrt{\frac{\hat{p}_j(1 - \hat{p}_j)}{n}}
$$

--

- Compute `\(\alpha\)`-level __confidence interval__ (CI) as `\(\hat{p}_j \pm z_{1 - \alpha / 2} \cdot SE(\hat{p}_j)\)`

  + In code: `\(z_{1 - \alpha / 2}\)` = `qnorm(1 - alpha / 2)`

- Good rule-of-thumb: construct 95% CI using `\(\hat{p}_j \pm 2 \cdot SE(\hat{p}_j)\)`

--

- Uses a Normal approximation justified by CLT (so CI could include values outside of [0,1]...):

$$
\sqrt{n} (\hat{p}_j - p_j) \rightarrow \text{Normal}(0,\ p_j(1 - p_j))
$$
- To get CI for counts: just multiply `\(\hat{p}\)` by `\(n\)`



---

## Add standard errors to bars

.pull-left[


```r
penguins %&gt;%
  group_by(species) %&gt;% 
  summarize(count = n(), .groups = "drop") %&gt;% 
  mutate(total = sum(count), 
         prop = count / total,
*        se = sqrt(prop * (1 - prop) / total),
*        lower = prop - 2 * se,
*        upper = prop + 2 * se) %&gt;%
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop),
           stat = "identity") +
* geom_errorbar(aes(ymin = lower,
*                   ymax = upper),
*               color = "red")
```


- If CIs don‚Äôt overlap `\(\rightarrow\)` likely significant difference

- If CIs overlap a little `\(\rightarrow\)` ambiguous

- If CIs overlap a lot `\(\rightarrow\)` no significant difference

]

.pull-right[

&lt;img src="figs/Lec3/unnamed-chunk-5-1.png" width="100%" /&gt;

]

---

## Don't do this...

&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;Wait a second, these are not real error bars ‚Ä¶ the author literally just put the letter ‚ÄúT‚Äù above the bar graphs üò≠ &lt;a href="https://t.co/KKtTGRHFaw"&gt;pic.twitter.com/KKtTGRHFaw&lt;/a&gt;&lt;/p&gt;&amp;mdash; Josemari Feliciano (@SeriFeliciano) &lt;a href="https://twitter.com/SeriFeliciano/status/1597355324008108034?ref_src=twsrc%5Etfw"&gt;November 28, 2022&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

---

## Back up a bit: Chi-Squared Test

- The chi-squared test is the main, high-level test for categorical data

- Determines if there's a significant difference between the __expected__ and __observed__ frequencies across categories _at a global level_

--

- 2D Categorical Data: Tests independent of two categorical variables (more on this next week)

- 1D Categorical Data: Test if proportions across categories are equal:

$$
`\begin{align}
\text{Null: } H_0:&amp; \ p_1 = p_2 = \cdot \cdot \cdot = p_K = \frac{1}{K}\\
\text{Alternative: } H_A:&amp; \text{ The probabilities are not all equal}
\end{align}`
$$

- note the alternative is __NOT__ the same as `\(p_1 \neq = p_2 \neq \cdot \cdot \cdot \neq p_K\)`

---

## Computing and Interpreting the Chi-Square Test

$$
\text{Test Statistic: }\chi^2 = \sum_{j = 1}^K \frac{(O_j - E_j)^2}{E_j}
$$

- `\(O_j\)`: Observed counts in category `\(C_j\)`

- `\(E_j\)`: Expected counts under `\(H_0\)`, i.e., each category is equally likely to occur `\(n/K = p_1 = \cdot \cdot \cdot = p_K\)`)

--

.pull-left[

```r
*chisq.test(table(penguins$species))
```

```
## 
## 	Chi-squared test for given probabilities
## 
## data:  table(penguins$species)
## X-squared = 31.907, df = 2, p-value = 1.179e-07
```

]
.pull-right[

#### Interpretation

&lt;!-- - Large `\(\chi^2 \rightarrow\)` observed counts are very different from expected counts --&gt;

&lt;!-- - Therefore we should just reject the null hypothesis --&gt;

&lt;!-- - i.e., the `\(p\)`-value is small because we would not expect to see observed counts so extreme if the null were true --&gt;

&lt;!-- - But if we reject, cannot tell _which_ probabilities are different... --&gt;

&lt;!-- - _Can we use graphs to tell us which are different...?_ --&gt;
]

---

# Hypothesis testing review

.pull-left[
Computing `\(p\)`-values works like this:

- Choose a test statistic.

- Compute the test statistic in your dataset.

- Is test statistic "unusual" compared to what I would expect under `\(H_0\)`?

- Compare `\(p\)`-value to __target error rate__ `\(\alpha\)` (typically referred to as target level `\(\alpha\)` )

- Typically choose `\(\alpha = 0.05\)` 

  - i.e., if we reject null  hypothesis at `\(\alpha = 0.05\)` then, assuming `\(H_0\)` is true, there is a 5% chance it is a false positive (aka Type 1 error)
]

--


.pull-right[

&lt;img src="https://measuringu.com/wp-content/uploads/2021/04/042121-F2.jpg" width="100%" style="display: block; margin: auto;" /&gt;


]


---

## Computing and Interpreting the Chi-Square Test

$$
\text{Test Statistic: }\chi^2 = \sum_{j = 1}^K \frac{(O_j - E_j)^2}{E_j}
$$

- `\(O_j\)`: Observed counts in category `\(C_j\)`

- `\(E_j\)`: Expected counts under `\(H_0\)`, i.e., each category is equally likely to occur `\(n/K = p_1 = \cdot \cdot \cdot = p_K\)`)

--

.pull-left[

```r
*chisq.test(table(penguins$species))
```

```
## 
## 	Chi-squared test for given probabilities
## 
## data:  table(penguins$species)
## X-squared = 31.907, df = 2, p-value = 1.179e-07
```

]
.pull-right[

- However, if we reject, we cannot tell _which_ probabilities are different...

- _Can we use graphs to tell us which are different...?_

]


---

## Graphics versus Statistical Inference

- Reminder - Anscombe's Quartet: where statistical inference was the same but the graphics were very different

&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/1200px-Anscombe%27s_quartet_3.svg.png" width="55%" style="display: block; margin: auto;" /&gt;


--

- __The opposite can be true!__ Graphics are the same, but statistical inference is very different...


---

### Example: 3 categories, `\(p_1 = 1/2,\ p_2 = p_3 = 1/4\)`

&lt;img src="figs/Lec3/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

### Example: 3 categories, `\(p_1 = 1/2,\ p_2 = p_3 = 1/4\)`

&lt;img src="figs/Lec3/low-count-noted-bars-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

### Example: 3 categories, `\(p_1 = 1/2,\ p_2 = p_3 = 1/4\)`

&lt;img src="figs/Lec3/mid-count-noted-bars-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

### Example: 3 categories, `\(p_1 = 1/2,\ p_2 = p_3 = 1/4\)`

&lt;img src="figs/Lec3/high-count-noted-bars-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## Power under this scenario: (2n/4, n/4, n/4)

&lt;img src="figs/Lec3/chi-power-curve-1.png" width="100%" style="display: block; margin: auto;" /&gt;


---

## How do we combine graphs with inference?

1. Simply add `\(p\)`-values (or other info) to graph via text

2. Add confidence intervals to the graph

  - Need to remember what each CI is for! 
  
  - Our CIs on previous slides are for each `\(\hat{p}_j\)` marginally, __NOT__ jointly

  - Have to be careful with __multiple testing__...

---

## CIs will visually capture uncertainty in estimates

.pull-left[

&lt;img src="figs/Lec3/low-count-bars-se-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]
.pull-right[

&lt;img src="figs/Lec3/high-count-bars-se-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---

## (Rough) Rules-of-thumb for comparing CIs on bar charts

- Comparing overlap of two CIs is __NOT__ exactly the same as directly testing for a significant difference...

  - Really you want CI( `\(\hat{p}_1 - \hat{p}_2\)` ), not CI( `\(\hat{p_1}\)` ) and CI( $\hat{p_2} $)
  
  - CI( `\(\hat{p_1}\)` ) and CI( `\(\hat{p_2}\)` ) not overlapping implies `\(0 \notin\)` CI( `\(\hat{p}_1 - \hat{p}_2\)` )
  
--

  - _However_ CI( `\(\hat{p_1}\)` ) and CI( `\(\hat{p_2}\)` ) overlaping __DOES NOT__ imply `\(0 \in\)` CI( `\(\hat{p}_1 - \hat{p}_2\)` ) 


--

Roughly speaking:

  - If CIs don't overlap `\(\rightarrow\)` significant difference
  
  - If CIs overlap a little `\(\rightarrow\)` ambiguous
  
  - If CIs overlap a lot `\(\rightarrow\)` no significant difference
  
--

But if we're comparing more than two CIs simultaneously, we need to account for __multiple testing__!

  - When you look for all non-overlapping CIs: implicitly making `\(\binom{K}{2} = \frac{K!}{2!(K-2)!}\)` pairwise tests in your head!
  

---

## Corrections for multiple testing (and visualizing them)

- In those bar plots, when we determine whether CIs overlap we make 3 comparisons:

  1. A vs B
  
  2. A vs C
  
  3. B vs C
  
#### This is a multiple testing issue

--

- In short: we will make Type 1 errors (chance of false rejecting) more than 5% of the time!

- Reminder: Type 1 error = Rejecting `\(H_0\)` when `\(H_0\)` is true

- e.g., CIs don't overlap but actually `\(H_0: p_A = p_B\)` is true
  
- If only interested in A vs B __and nothing else__, then just construct 95% CI for A vs B and _control error rate_ at 5%
  
- However, if we construct several CIs, where A vs B is just one comparison we make, our Type 1 error rate &gt; 5%!

---

## Corrections for multiple testing (and visualizing them)

Vast literature on corrections for multiple testing (beyond the scope of this class... but in my thesis!)

- But you should understand the following:

1. Corrections for multiple testing inflate `\(p\)`-values (i.e., make them bigger)

2. Equivalently, they inflate CIs (i.e., make them wider)

3. Purpose of these corrections is to control Type 1 error rate `\(\leq 5\%\)`

--

We'll focus on the __Bonferroni correction__, which inflates `\(p\)`-values the most but is easy to implement and very popular:

+ We usually reject null hypothesis when `\(p\)`-value `\(\leq .05\)`

+ __Bonferroni__: if making `\(K\)` comparisons, reject only if `\(p\)`-value `\(\leq .05/K\)`

--

+ For CIs: instead of plotting 95% CIs, we plot (1 - `\(0.05/K\)`)% CIs

  + e.g., for `\(K = 3\)` then plot 98.3% CIs
  
---

## Impact of Bonferroni correction on CIs...

.pull-left[
&lt;img src="figs/Lec3/unnamed-chunk-11-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.pull-right[

&lt;img src="figs/Lec3/larger-intervals-1.png" width="100%" style="display: block; margin: auto;" /&gt;

]

---

## Main Takeaways

+ Graphs looking at 1D categorical data (e.g., bar charts) look at the empirical distribution of the categorical variable ( `\(\hat{p}_1, \dots, \hat{p}_K\)` )

+ Chi-squared test is the main statistical test for 1D categorical data (_global test_), testing `\(H_0 : p_1 = \cdot \cdot \cdot p_K\)`

+ However, from this test alone, we can‚Äôt tell which probabilities differ

+  Can generate individual CIs for each `\(\hat{p}_1\)`, `\(\dots\)`, `\(\hat{p}_K\)`

  +  Allows for easy visualization.
  
  + But can be complicated, especially with respect to multiple testing.
  
+ Graphs with the same trends can display very different statistical significance (largely due to sample size)

  
---

## Useful to order categories by frequency with [`forcats`](https://forcats.tidyverse.org/)

.pull-left[


```r
penguins %&gt;%
  group_by(species) %&gt;% 
  summarize(count = n(), .groups = "drop") %&gt;% 
  mutate(total = sum(count), 
         prop = count / total,
         se = sqrt(prop * (1 - prop) / total), 
         lower = prop - 2 * se, 
         upper = prop + 2 * se,
         species = 
*          fct_reorder(species, prop)) %&gt;%
  ggplot(aes(x = species)) +
  geom_bar(aes(y = prop),
           stat = "identity") +
  geom_errorbar(aes(ymin = lower, 
                    ymax = upper), 
                color = "red") 
```


]

.pull-right[

&lt;img src="figs/Lec3/unnamed-chunk-12-1.png" width="100%" /&gt;

]



---
class: center, middle

# Next time: 2D categorical data

### HW1 is posted and due Wednesday Feb 1 by 11:59 PM

Recommended reading: 

[CW Chapter 16.2 Visualizing the uncertainty of point estimates](https://clauswilke.com/dataviz/visualizing-uncertainty.html#visualizing-the-uncertainty-of-point-estimates)





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
