---
title: "36-315 Lecture 15"
subtitle: "Visualizing Distances for High-Dimensional Data"  
author: 
  - "Professor Ron Yurko"
date: '3/20/2023'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  fig.path = "figs/Lec15/"
)

xaringanExtra::use_clipboard()
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#c41230",
  secondary_color = "#0277BD",
  inverse_header_color = "#FFFFFF"
)
```


```{r, include = FALSE}
library(tidyverse)
```

# Conceptual review

Last class: contour plots, heat maps, and diving into high-dimensional data

#### Today: how do we visualize structure of high-dimensional data?

- Example: What if I give you a dataset with 50 variables, and ask you to make __one visualization__ that best represents the data? _What do you do?_

--

- Do NOT panic and make $\binom{50}{2} = 1225$ pairs of plots!

- __Intuition__: Take high-dimensional data and __represent it in 2-3 dimensions__, then visualize those dimensions


---

## What about high-dimensional data?

Consider this [dataset]((https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-12-21/readme.md) containing nutritional information about Starbucks drinks:

```{r, warning = FALSE, message = FALSE}
starbucks <- 
  read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv") %>%
  # Convert columns to numeric that were saved as character
  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))
starbucks %>% slice(1)
```


#### Visualize structure among observations using distances matrices

---

## Thinking about distance...

When describing visuals, we've implicitly "clustered" observations together

- e.g., where are the mode(s) in the data?

--

These types of task require characterizing the __distance__ between observations

- Clusters: groups of observations that are "close" together

--

This is easy to do for 2 quantitative variables: just make a scatterplot (possibly with contours or heatmap)

#### But how do we define "distance" for high-dimensional data?

--

Let $\boldsymbol{x}_i = (x_{i1}, \dots, x_{ip})$ be a vector of $p$ features for observation $i$

Question of interest: How "far away" is $\boldsymbol{x}_i$ from $\boldsymbol{x}_j$?

--

When looking at a scatterplot, you're using __Euclidean distance__ (length of the line in $p$-dimensional space):

$$d(\boldsymbol{x}_i, \boldsymbol{x}_j) = \sqrt{(x_{i1} - x_{j1})^2 + \dots + (x_{ip} - x_{jp})^2}$$

---

## Distances in general

There's a variety of different types of distance metrics: [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry), [Mahalanobis](https://en.wikipedia.org/wiki/Mahalanobis_distance), [Cosine](https://en.wikipedia.org/wiki/Cosine_similarity), [Kullback-Leiber Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [Wasserstein](https://en.wikipedia.org/wiki/Wasserstein_metric), but we're just going to focus on [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)

--

$d(\boldsymbol{x}_i, \boldsymbol{x}_j)$ measures pairwise distance between two observations $i,j$ and has the following properties:

1. __Identity__: $\boldsymbol{x}_i = \boldsymbol{x}_j \iff d(\boldsymbol{x}_i, \boldsymbol{x}_j) = 0$

2. __Non-Negativity__: $d(\boldsymbol{x}_i, \boldsymbol{x}_j) \geq 0$

3. __Symmetry__: $d(\boldsymbol{x}_i, \boldsymbol{x}_j) = d(\boldsymbol{x}_j, \boldsymbol{x}_i)$

4. __Triange Inequality__: $d(\boldsymbol{x}_i, \boldsymbol{x}_j) \leq d(\boldsymbol{x}_i, \boldsymbol{x}_k) + d(\boldsymbol{x}_k, \boldsymbol{x}_j)$

--

.pull-left[

__Distance Matrix__: matrix $D$ of all pairwise distances

- $D_{ij} = d(\boldsymbol{x}_i, \boldsymbol{x}_j)$

- where $D_{ii} = 0$ and $D_{ij} = D_{ji}$

]
.pull-right[

$$D = \begin{pmatrix}
                0 & D_{12} & \cdots & D_{1n} \\
                D_{21} & 0 & \cdots & D_{2n} \\
                \vdots & \vdots & \ddots & \vdots \\
                D_{n1} & \cdots & \cdots & 0
            \end{pmatrix}$$

]

---

## What could go wrong with Euclidean distance?

---

## Multi-dimensional scaling (MDS)

#### General approach for visualizing distance matrices

- Puts $n$ observations in a $k$-dimensional space such that the distances are preserved as much as possible

  - where $k << p$ typically choose $k = 2$
  
--

MDS attempts to create new point $\boldsymbol{y}_i = (y_{i1}, y_{i2})$ for each unit such that:

$$\sqrt{(y_{i1} - y_{j1})^2 + (y_{i2} - y_{j2})^2} \approx D_{ij}$$

- i.e., distance in 2D MDS world is approximately equal to the actual distance

--

#### Then plot the new $\boldsymbol{y}$s on a scatterplot

- Use the `scale()` function to ensure variables are comparable

- Make a distance matrix for this dataset

- Visualize it with MDS

---

## MDS workflow example with Starbucks drinks

.pull-left[

```{r starbucks-mds, eval = FALSE}
starbucks_scaled_quant_data <- starbucks %>%
  dplyr::select(serv_size_m_l:caffeine_mg) %>%
  scale(center = FALSE, 
        scale = apply(., 2, sd, na.rm = TRUE)) #<<

dist_euc <- dist(starbucks_scaled_quant_data) #<<

starbucks_mds <- cmdscale(d = dist_euc, k = 2) #<<

starbucks <- starbucks %>%
  mutate(mds1 = starbucks_mds[,1],
         mds2 = starbucks_mds[,2])

starbucks %>%
  ggplot(aes(x = mds1, y = mds2)) +
  geom_point(alpha = 0.5) +
  labs(x = "Coordinate 1", y = "Coordinate 2")
```

]

.pull-right[

```{r ref.label="starbucks-mds", fig.height=7, echo = FALSE}

```

]

---

## What does `dist()` return?

```{r}
dist(starbucks_scaled_quant_data[1:8,]) #<<
```


#### Default distance calculation is Euclidean

---

## What does `cmdscale` do?

```{r, eval = FALSE}
starbucks_mds <- cmdscale(d = dist_euc, k = 2) #<<
```


`cmdscale()` is the function we use to run MDS and it has two inputs: 

1. `d`: distance matrix, e.g., `dist_euc`

2. `k`: number of dimensions we want, e.g., usually 2 for visualization purposes

Input is $N \times N$ matrix, and the output is $N \times 2$

--

To grab the output, we just grab the two columns of `starbucks_mds` and then can
make a scatterplot of these two new dimensions

```{r, eval = FALSE}
starbucks <- starbucks %>%
  mutate(mds1 = starbucks_mds[,1], #<<
         mds2 = starbucks_mds[,2]) #<<
```


---

## Interpreting the 2D projection

```{r, fig.align='center', echo = FALSE, fig.height = 4}
starbucks %>%
  ggplot(aes(x = mds1, y = mds2)) +
  geom_point(alpha = 0.5) +
  labs(x = "Coordinate 1", y = "Coordinate 2") +
  coord_fixed() 
```


---

## View structure with additional variables - `size`


```{r, fig.align='center', echo = FALSE, fig.height = 4}
starbucks %>%
  ggplot(aes(x = mds1, y = mds2, color = size)) +
  geom_point(alpha = 0.5) +
  labs(x = "Coordinate 1", y = "Coordinate 2") +
  coord_fixed() 
```


---

## View structure with additional variables - `sugar_g`


```{r, fig.align='center', echo = FALSE, fig.height = 4}
starbucks %>%
  ggplot(aes(x = mds1, y = mds2, color = sugar_g)) +
  geom_point(alpha = 0.5) +
  labs(x = "Coordinate 1", y = "Coordinate 2") +
  scale_color_gradient(low = "darkblue", high = "darkorange") +
  coord_fixed() 
```



---

## Main Takeaways

When data is high dimensional, it's impossible to visualize _every_ dimension in the data, so instead:

#### We reduce the data to a small number of dimensions, and then plot those dimensions

1. __Compute a distance matrix__: reduces the data to a single distance between points

2. __Run Multi-Dimensional Scaling (MDS)__: summarizes the distance matrix in 2 or 3 dimensions

3. __Plot the dimensions provided by MDS__

Adding other dimensions (e.g., via color) when plotting MDS can be a great way to see clusters in the data

--

+ __HW5 is due Wednesday March 22nd!__

+ __Graphics critique due April 7th!__

+ Review more code in Lecture 15 Demo! 


#### Next time: Dendrograms to visualize distances and clusters


