---
title: "36-315 Lecture 12"
subtitle: "More Regression and Midsemester Review"  
author: 
  - "Professor Ron Yurko"
date: '2/27/2023'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  fig.path = "figs/Lec12/"
)

xaringanExtra::use_clipboard()
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#c41230",
  secondary_color = "#0277BD",
  inverse_header_color = "#FFFFFF"
)
```


```{r, include = FALSE}
library(tidyverse)
library(palmerpenguins)
```


## More fun with `penguins`...

Lecture 11 Demo: Walk through an example of plotting/running different linear regression models

+ __Outcome__: bill depth (in mm)

+ __Covariates__: bill length (depth) and species 

Linear regression models we will consider:

1. `bill_depth_mm` ~ `bill_length_mm`

2. `bill_depth_mm` ~ `bill_length_mm` + `species`

3. `bill_depth_mm` ~ `bill_length_mm` + `species` + `bill_length_mm` $\times$ `species`


---

## Model #1: 
### `bill_depth_mm` ~ `bill_length_mm`

```{r, echo = FALSE, fig.align='center'}
penguins %>%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE)
```


---

## Special Case - Categorical Variables

Can include categorical variables in multiple linear regression, but need to code them as "dummy variables" (i.e., indicator variables)

Say a categorical variable has $k \geq 2$ levels. Need to create $(k-1)$ indicator variables, equal to 1 for _one_ category and 0 otherwise

__Important__: Categorical variable may be coded numerically (e.g., Agree = 1, Disagree = -1, Not Sure = 0)

- If you put this variable straight into `lm()`, it will fit a very different model!

---

## Understanding the Categorical Variables Example

Example: Penguins species: _Adelie_, _Chinstrap_, _Gentoo._ There are $k = 3$ levels.

Create an indicator for _Chinstrap_ and _Gentoo_: $I_C$ and $I_G$.  

- If $I_C = I_G = 0$, then the penguin must be _Adelie_

The statistical model would be $Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_C I_C + \beta_G I_G, \sigma^2)$

--

- $\beta_0$: Mean for _Adelie_

- $\beta_0 + \beta_C$: Mean for _Chinstrap_

- $\beta_0 + \beta_G$: Mean for _Gentoo_

--

- Significant $\beta_C$ $\rightarrow$ Chinstrap and Adelie are different

- Significant $\beta_G$ $\rightarrow$ Gentoo and Adelie are different

- How to compare Chinstrap and Gentoo? Need to fit a different model.

---

## Understanding Interactions (Categorical Example Again)

Say we also have a quantitative variable $X$ (bill length). Consider two statistical models:

1. $Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_X X + \beta_C I_C + \beta_G I_G, \sigma^2)$

2. $Y_i \stackrel{iid}{\sim} N(\beta_0 + \beta_X X + \beta_C I_C + \beta_G I_G + \beta_{CX} I_C X + \beta_{GX} I_G X, \sigma^2)$

--

For Model 1...

- The intercept for Adelie is $\beta_0$; for Chinstrap it is $\beta_0 + \beta_C$; for Gentoo it is $\beta_0 + \beta_G$

- The slope for all species is $\beta_X$.

--

For Model 2...

- The intercept for Adelie is $\beta_0$; for Chinstrap it is $\beta_0 + \beta_C$; for Gentoo it is $\beta_0 + \beta_G$.

- The slope for Adelie is $\beta_X$; for Chinstrap it is $\beta_X + \beta_{CX}$; for Gentoo it is $\beta_X + \beta_{GX}$

--

Significant coefficient for categorical variables by themselves? Significantly different intercepts

Significant coefficient for interactions with categorical variables? Significantly different slopes

---

## Model #2: 
### `bill_depth_mm` ~ `bill_length_mm` + `species`

```{r, echo = FALSE, fig.align='center'}
depth_lm_species_add <- lm(bill_depth_mm ~ bill_length_mm + species,
                           data = penguins)

# Calculate species-specific intercepts in order:
intercepts <- # First for `Adelie` it's just the initial intercept
  c(coef(depth_lm_species_add)["(Intercept)"],
    # Next for `Chinstrap` it's the intercept plus the `Chinstrap` term:
    coef(depth_lm_species_add)["(Intercept)"] + 
      coef(depth_lm_species_add)["speciesChinstrap"],
    # And finally for `Gentoo` it's again the intercept plus the `Gentoo` term
    coef(depth_lm_species_add)["(Intercept)"] + 
      coef(depth_lm_species_add)["speciesGentoo"])

# Create a small table to store the intercept, slopes, and species:
lines_tbl <- tibble("intercepts" = intercepts,
                    # Slopes are the same for each, thus use rep()
                    "slopes" = rep(coef(depth_lm_species_add)["bill_length_mm"],
                                   3),
                    # And the levels of species:
                    "species" = levels(penguins$species))
penguins %>%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point(alpha = 0.5) +
  geom_abline(data = lines_tbl,
              aes(intercept = intercepts, slope = slopes,
                  color = species)) +
  labs(x = "Bill length (mm)", y = "Bill depth (mm)", 
       title = "Bill depth versus weight by species")
```

---

```{r}
summary(lm(bill_depth_mm ~ bill_length_mm + species, data = penguins))
```


---

## Model #3: 
### `bill_depth_mm` ~ `bill_length_mm` + `species` + `bill_length_mm` $\times$ `species`

```{r, echo = FALSE, fig.align='center'}
penguins %>%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = TRUE) +
  labs(x = "Bill length (mm)", y = "Bill depth (mm)", 
       title = "Bill depth versus weight by species")
```


---

```{r}
summary(lm(bill_depth_mm ~ bill_length_mm * species, data = penguins))
```


---

## A Few Linear Regression Warnings


#### Simpson's Paradox

- There is a negative linear relationship between two variables but a positive linear relationship within every subpopulation

- In these cases, subgroup analysis is especially important

--

#### Is the intercept meaningful?

- Think about whether $X = 0$ makes scientific sense for a particular variable before you interpret the intercept

--

#### Interpolation versus Extrapolation

- Interpolation is defined as prediction within the range of a variable

- Extrapolation is defined as prediction outside the range of a variable

- Generally speaking, interpolation is more reliable than extrapolation. (Less sensitive to model misspecification.)


---

## Extrapolation Example

```{r, echo = FALSE, fig.align='center'}
set.seed(1389)
fake_data <- tibble(x = runif(100, min = -10, max = -3)) %>%
  mutate(y = rnorm(n = 100, mean = 3 * x^2, sd = 3))
fake_data %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

---

## Extrapolation Example

```{r, echo = FALSE, fig.align='center'}
fake_data %>%
  ggplot(aes(x, y)) +
  geom_point() +
  scale_x_continuous(limits = c(-11, 11)) +
  geom_smooth(method = "lm") +
  theme_bw()
```


---

## Extrapolation Example

```{r, echo = FALSE, fig.align='center'}
fake_data %>%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(-11, 11)) +
  stat_function(
    fun = function (x) 3 * x^2,
    linetype = "dashed", color = "red") +
  theme_bw()
```


---

## Take-home exam logistics

#### I will post it today at 2:30 PM ET, due Wednesday 11:59 PM ET on Gradescope

While the exam is in progress...

+ You cannot talk to anyone else about 36-315.

+ You cannot post on Piazza.

+ You _can_ use any materials that are available to you (lectures, labs, homeworks, R demos).

One-on-one exam office hours: Wednesday 12:00 to 12:50 PM, then 1:30 to 2:30 EST (zoom to be posted on Canvas)

+ __Only__ for clarifying questions about what is being asked on the exam.


Have a question but can't come to these office hours? __Email me!__

Best way to prepare:

+ Look over lecture notes, R demos, homework/lab solutions

+ Come to office hours!

---

## The Main Skills I've Wanted You to Learn

#### Pick graph types that are most appropriate for a particular dataset

+ Requires a working knowledge of different graph types and need to appropriately distinguish categorical vs quantitative variables.

+ For any graph, need to know what information is visible vs hidden

--

#### Characterizing distributions (visually and quantitatively)

+ Need a "distributional vocabulary" (center/mode, spread, skewness) and need to choose graphs that showcase distributional quantities 

+ Need to choose graph _specifications_ that showcase distributional quantities (e.g., binwidth/bandwidth)

--

#### Conduct statistical inference to complement graphs

+ For most differences you spot in a graph, should be able to follow-up with an analysis to determine if that difference is significant

+ Requires a working knowledge of different statistical tests

+ Need to know how to interpret the output from statistical tests (knowing the null/alternative hypotheses is key!)


---

## Variable Types

The first thing you should do when you look at a dataset is determine what the variable types are.

__Categorical__: May have order (ordinal) or no order (nominal).

+ Often represented as a `factor` in `R`

+ May be coded with numbers!

+ If only 3-5 values, probably appropriate to treat as categorical.

__Quantitative__: Represented numerically. Always has order.

+ Represented as `numeric` or `integer` in `R`.


How to determine if a variable is quantitative or categorical?

+ Often obvious, but not always.=

+ _Subtraction test_: Does $X_1 - X_2$ lead to a sensible value? If so, it's quantitative.

+ If a variable is used in scatterplots/regression, it shouldn't have a super strict range. 1-to-5 Likert scale variables fail this.

---

## Variable Type Situations


---

## Variable Type Situations



---

## Statistical Tests/Analyses

__Chi-square test for equal proportions__: $H_0: p_1 = \cdots = p_K$.

__Chi-square test for independence__: $H_0:$ Variables are independent.

+ Dependence: $P(A | B) \neq P(A)$

--

__One-sample KS test__: $H_0$: Variable follows a distribution.

__t-test/ANOVA__: $H_0$: Group means equal.

__Bartlett's test__: $H_0$: Group variances equal.

__Two-Sample KS Test__: $H_0$: Variables follow the same distribution.

--

__Linear Regression__: $H_0: \beta = 0$

+ Need to distinguish between intercepts and slopes!


#### Remember: Different tests have different _power_ (chance of rejecting $H_0$ when you should)


---

### Distribution Terminology

__Marginal Distributions__: $P(A)$ - plot a graph of a single variable $A$.

+ Perhaps compare confidence intervals for different categories of $A$.

--

__Conditional Distributions__: $P(A | B)$ - in English: Distribution of $A$ given a particular value of $B$.

+ Goal: Compare $P(A | B = b)$ for different $b$ when $A$ is quantitative and $B$ categorical

+ A univariate graph (histograms, densities, violins) for each category.

+ When $A$ and $B$ are categorical, can visualize with stacked bar plots or mosaic plots.

+ Note: Linear regression estimates $\mathbb{E}[Y | X]$

--

__Joint Distribution__: $P(A, B)$

+ Use mosaic plots when $A$ and $B$ are categorical.

+ $P(A | B) P(B) = P(A, B)$

+ Scatterplots display joint distribution for continuous.


---

### Good luck!

```{r, echo = FALSE, fig.align='center'}
knitr::include_graphics("https://media.tenor.com/jLWC97XT1ZIAAAAC/star-wars-han-solo.gif")
```



---
class: center, middle

# TAKE-HOME EXAM IS DUE WEDNESDAY!

Reminder: __Graphics critique due tomorrow night!__

Please fill out this midsemester survey _after_ the exam: https://forms.gle/Rqzf4Ptx4X1CZXJX9

