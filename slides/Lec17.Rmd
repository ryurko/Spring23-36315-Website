---
title: "36-315 Lecture 17"
subtitle: "More Dendrograms and Principal Component Analysis"  
author: 
  - "Professor Ron Yurko"
date: '3/27/2023'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE,
  fig.path = "figs/Lec17/"
)

xaringanExtra::use_clipboard()
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#c41230",
  secondary_color = "#0277BD",
  inverse_header_color = "#FFFFFF"
)
```


```{r, include = FALSE}
library(tidyverse)
starbucks <- 
  read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-12-21/starbucks.csv") %>%
  # Convert columns to numeric that were saved as character
  mutate(trans_fat_g = as.numeric(trans_fat_g), fiber_g = as.numeric(fiber_g))

```


## Lecture 16 Demo: [MCU movie data](https://informationisbeautiful.net/visualizations/which-is-the-best-performing-marvel-movie/)

```{r, echo = FALSE, fig.align='center', fig.height = 4}
mcu_movies <- read_csv("https://raw.githubusercontent.com/ryurko/Spring23-36315-Data/master/mcu_movies.csv")

mcu_quant <- mcu_movies %>% 
  dplyr::select(-c(film, category, year))

mcu_quant <- apply(mcu_quant, MARGIN = 2,
                   FUN = function(x) x / sd(x))
rownames(mcu_quant) <- mcu_movies$film

mcu_dist <- dist(mcu_quant)

mcu_mds <- cmdscale(d = dist(mcu_quant), k = 2)

# Add to original dataset
mcu_movies <- mcu_movies %>%
  mutate(mds1 = mcu_mds[,1], 
         mds2 = mcu_mds[,2])

mcu_movies %>%
  ggplot(aes(x = mds1, y = mds2)) +
  # Use text labels instead of points:
  geom_text(aes(label = film),
            alpha = .75) +
  labs(x = "MDS Coordinate 1", y = "MDS Coordinate 2") +
  theme_bw()
```


---

## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)

.pull-left[
Start with all observations in their own cluster

- Step 1: Compute the pairwise dissimilarities between each cluster

- Step 2: Identify the pair of clusters that are __least dissimilar__

- Step 3: Fuse these two clusters into a new cluster!

- __Repeat Steps 1 to 3 until all observations are in the same cluster__
]
.pull-right[
```{r out.width='85%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png")
```

Forms a __dendrogram__ (typically displayed from bottom-up)
]

---

## How do we define dissimilarity between clusters?

We know how to compute distance / dissimilarity between two observations

__But how do we handle clusters?__

  - Dissimilarity between a cluster and an observation, or between two clusters

We need to choose a __linkage function__! Clusters are built up by __linking them together__

Compute all pairwise dissimilarities between observations in cluster 1 with observations in cluster 2

i.e. Compute the distance matrix between observations, $d(x_i, x_j)$ for $i \in C_1$ and $j \in C_2$


  - __Complete linkage__: Use the __maximum__ value of these dissimilarities: $\underset{i \in C_1, j \in C_2}{\text{max}} d(x_i, x_j)$
  

  - __Single linkage__: Use the __minimum__ value: $\underset{i \in C_1, j \in C_2}{\text{min}} d(x_i, x_j)$


  - __Average linkage__: Use the __average__ value: $\frac{1}{|C_1| \cdot |C_2|} \sum_{i \in C_1} \sum_{j \in C_2} d(x_i, x_j)$
  

Define dissimilarity between two clusters __based on our initial dissimilarity matrix between observations__


---

```{r, fig.align='center', fig.height=7}
hc_complete <- hclust(mcu_dist, method = "complete")
plot(hc_complete, ylab = "Pairwise Distance", main = "Complete Linkage", xlab = "MCU Movies")
```


---

```{r, fig.align='center', fig.height=7}
hc_single <- hclust(mcu_dist, method = "single")
plot(hc_single, ylab = "Pairwise Distance", main = "Single Linkage", xlab = "MCU Movies")
```


---

#### [`ggdendro` version](https://cran.r-project.org/web/packages/ggdendro/vignettes/ggdendro.html)

```{r, fig.align='center'}
library(ggdendro)
ggdendrogram(hc_complete, theme_dendro = FALSE) + #<<
  labs(y = "Cluster Dissimilarity (based on complete linkage)", 
       title = "Which MCU movies are similar to each other?") + 
  coord_flip() + theme_bw() + theme(axis.title.y = element_blank())
```


---

### Useful to display MDS plot with dendrogram side-by-side

```{r, echo = FALSE, fig.align='center'}
library(patchwork)
hc_complete_ggdendro <- ggdendrogram(hc_complete, theme_dendro = FALSE) +
  labs(y = "Cluster Dissimilarity (based on complete linkage)", 
       title = "Which MCU movies are similar to each other?") + 
  coord_flip() +
  theme_bw() +
  # Remove the y-axis title (changed from x to y since we flipped it!)
  theme(axis.title.y = element_blank(),
        axis.text.y = element_text(size = 6),
        plot.title = element_text(size = 10))

mcu_mds_plot <- mcu_movies %>%
  ggplot(aes(x = mds1, y = mds2)) +
  geom_text(aes(label = film),
            alpha = .75, size = 2) +
  labs(x = "MDS Coordinate 1", y = "MDS Coordinate 2") +
  theme_bw()

mcu_mds_plot + hc_complete_ggdendro
```


---

## How do we assign cluster labels?

#### We cut the dendrogram to return cluster labels

Two ways to specify how to cut the tree using the `cutree` function:

1) via the height using `h`, e.g., cut the tree at height = 10

```{r, eval = FALSE}
cutree(hc_complete, h = 10)
```


2) via the desired number of clusters `k` - and let the computer figure out the height for us, e.g., `k = 2`

```{r, eval = FALSE}
cutree(hc_complete, k = 2)
```


---

```{r}
mcu_clusters <- cutree(hc_complete, h = 10)
mcu_clusters
```


---

### View results with cut on dendrogram

```{r, echo = FALSE, fig.align='center'}
cut_dendro <- hc_complete_ggdendro +
  # This is a horizontal line since its considered before the flip:
  geom_hline(yintercept = 10, linetype = "dashed", 
             color = "darkred")

cluster_mcu_mds_plot <- mcu_movies %>%
  mutate(cluster = as.factor(mcu_clusters)) %>%
  ggplot(aes(x = mds1, y = mds2,
             color = cluster)) +
  geom_text(aes(label = film),
            alpha = .75, size = 2) +
  ggthemes::scale_color_colorblind() +
  labs(x = "MDS Coordinate 1", y = "MDS Coordinate 2") +
  theme_bw() +
  theme(legend.position = "bottom")

cluster_mcu_mds_plot + cut_dendro
```


---

### [`factoextra` package](https://rpkgs.datanovia.com/factoextra/index.html) version

```{r, warning = FALSE, message = FALSE, fig.align='center'}
library(factoextra)
fviz_dend(hc_complete, cex = 0.5, k = 3, color_labels_by_k = TRUE)
```


---

## Main Takeaways

#### Dendrograms are a great way to visualize distances and the clustering structure in the dataset

#### However there are several decisions to be made! 

#### What type of linkage is appropriate for the problem? 

#### How do we [choose the number of clusters](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/)? 

#### There is NOT a one size fits all solution to any of this!



---

## Conceptual review

#### We've been considering visuals for high-dimensional data

Common workflow:

+ Reduce the data to a few "useful" dimensions

+ Plot those "useful" dimensions


Last two classes:

1. Reduce the data by summarizing pairs of subjects with one distance.

2. Visualize distances using multi-dimensional scaling or dendrograms.

How can we reduce the data without distances?

#### Principal Component Analysis (PCA) is by far the most popular way


---

## Dimension reduction - searching for variance

__GOAL__: Focus on reducing dimensionality of feature space, i.e., number of columns, while __retaining__ most of the information, i.e., __variance__, in a lower dimensional space

- $n \times p$ matrix $\rightarrow$ dimension reduction technique $\rightarrow$ $n \times k$ matrix

--

Special case we just discussed: __MDS__

- $n \times n$ __distance__ matrix $\rightarrow$ MDS $\rightarrow$ $n \times k$ matrix (usually $k = 2$)

- This requires converting data into a distance matrix - summarizing all differences between observations into a single number, effectively "double reduction"

1. Reduce data to a distance matrix

2. Reduce distance matrix to $k = 2$ dimensions

#### How can we apply dimension to the original data?

---

## Principal Component Analysis (PCA)

$$
\begin{pmatrix}
& & \text{really} & & \\
& & \text{wide} & & \\
& & \text{matrix} & &
\end{pmatrix}
\rightarrow \text{matrix algebra stuff} \rightarrow 
\begin{pmatrix}
\text{much}  \\
\text{thinner}  \\
\text{matrix} 
\end{pmatrix}
$$

- Start with $n \times p$ matrix of __correlated__ variables $\rightarrow$ $n \times k$ matrix of __uncorrelated__ variables

--

- Each of the $k$ columns in the right-hand matrix are __principal components__, all uncorrelated with each other

- First column accounts for most variation in the data, second column for second-most variation, and so on

#### Intuition: first few principal components account for most of the variation in the data

---

## What are principal components?

- Assume $\boldsymbol{X}$ is a $n \times p$ matrix that is __centered__ and __stardardized__

- _Total variation_ $= p$, since Var( $\boldsymbol{x}_j$ ) = 1 for all $j = 1, \dots, p$

- PCA will give us $p$ principal components that are $n$-length columns - call these $Z_1, \dots, Z_p$

--

__First principal component__ (aka PC1):

$$Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1} X_p$$


--

  - $\phi_{j1}$ are the weights indicating the contributions of each variable $j \in 1, \dots, p$
  
  - Weights are normalized $\sum_{j=1}^p \phi_{j1}^2 = 1$
  
  - $\phi_{1} = (\phi_{11}, \phi_{21}, \dots, \phi_{p1})$ is the __loading vector__ for PC1

--
  
  - $Z_1$ is a linear combination of the $p$ variables that has the __largest variance__

---

## What are principal components?

__Second principal component__:

$$Z_2 = \phi_{12} X_1 + \phi_{22} X_2 + \dots + \phi_{p2} X_p$$

  - $\phi_{j2}$ are the weights indicating the contributions of each variable $j \in 1, \dots, p$
  
  - Weights are normalized $\sum_{j=1}^p \phi_{j1}^2 = 1$
  
  - $\phi_{2} = (\phi_{12}, \phi_{22}, \dots, \phi_{p2})$ is the __loading vector__ for PC2
  
  - $Z_2$ is a linear combination of the $p$ variables that has the __largest variance__
  
    - __Subject to constraint it is uncorrelated with $Z_1$__ 

--

We repeat this process to create $p$ principal components

- __Uncorrelated__: Each ( $Z_j, Z_{j'}$ ) is uncorrelated with each other

- __Ordered Variance__: Var( $Z_1$ ) $>$ Var( $Z_2$ ) $> \dots >$ Var( $Z_p$ )

- __Total Variance__: $\sum_{j=1}^p \text{Var}(Z_j) = p$


#### Intuition: pick some $k << p$ such that if $\sum_{j=1}^k \text{Var}(Z_j) \approx p$, then just using $Z_1, \dots, Z_k$

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

```{r out.width="100%", echo = FALSE, fig.align='center'}
# Ensure reproducibility by setting random number seed
set.seed(123) 
plot_data <- tibble("x" = rnorm(50, mean = 100, sd = 20)) %>%
  mutate(y =  0.8 * x + rnorm(50, mean = 0, sd = 10))
basic_scatter <- ggplot(plot_data) +
  geom_point(aes(x, y), color = "black")+
  coord_equal() +
  theme_bw()
basic_scatter
```

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

```{r out.width="100%", echo = FALSE, fig.align='center'}
#fit the model
line1 <- lm(y ~ x, plot_data)$coef
#extract the slope from the fitted model
line1.slope <- line1[2]
#extract the intercept from the fitted model
line1.intercept <- line1[1]
basic_scatter_yfit <- basic_scatter +
  geom_abline(aes(slope = line1.slope, intercept = line1.intercept),
              colour = "darkred") +
  annotate("text", x = 75, y = 120, label = "y ~ x", color = "darkred",
           size = 9)
basic_scatter_yfit
```

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

```{r out.width="100%", echo = FALSE, fig.align='center'}
#fit the model
line2 <- lm(x ~ y, plot_data)$coef
#extract the slope from the fitted model
line2.slope <- 1 / line2[2]
#extract the intercept from the fitted model
line2.intercept <- -(line2[1] / line2[2])
basic_scatter_xyfit <- basic_scatter_yfit +
  geom_abline(aes(slope = line2.slope, intercept = line2.intercept),
              colour = "blue") +
  annotate("text", x = 125, y = 55, label = "x ~ y", color = "blue",
           size = 9)
basic_scatter_xyfit
```

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

```{r out.width="100%", echo = FALSE, fig.align='center'}
pca <- prcomp(cbind(plot_data$x, plot_data$y))$rotation
pca.slope <- pca[2,1] / pca[1,1]
pca.intercept <- mean(plot_data$y) - (pca.slope * mean(plot_data$x))

basic_scatter_xy_pca_fit <- basic_scatter_xyfit +
  geom_abline(aes(slope = pca.slope, intercept = pca.intercept),
              colour = "darkorange") +
  annotate("text", x = 75, y = 90, label = "PCA", color = "darkorange",
           size = 9)
basic_scatter_xy_pca_fit
```

---

## [Visualizing PCA](https://www.stevejburr.com/post/scatter-plots-and-best-fit-lines/) in two dimensions

```{r out.width="100%", echo = FALSE, fig.align='center'}
plot_data %>%
  #calculate the positions using the line equations:
  mutate(yhat_line1=(x*line1.slope+line1.intercept),
         xhat_line1=x,
         yhat_line2=y,
         xhat_line2=(y-line2.intercept)/line2.slope,
         #https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line
         a=pca.slope,
         b=-1,
         c=pca.intercept,
         xhat_line3=(b*(b*x-a*y)-(a*c))/((a*a)+(b*b)),
         yhat_line3=(a*(-b*x+a*y)-(b*c))/((a*a)+(b*b)),
         #add the slopes/intercepts to this data frame:
         slope_line1=line1.slope,
         slope_line2=line2.slope,
         slope_line3=pca.slope,
         intercept_line1=line1.intercept,
         intercept_line2=line2.intercept,
         intercept_line3=pca.intercept
         )%>% 
  #drop intermediate variables
  select(-c(a,b,c)) %>%
  pivot_longer(yhat_line1:intercept_line3, names_to = "key", values_to = "value") %>%
  #transpose to a long form
  #gather(key="key",value="value",-c(x,y)) %>% 
  # have "yhat_line1", want two colums of "yhat" "line1"
  separate(key,c("type", "line"), "_") %>% 
  #then transpose to be fatter, so we have cols for xhat, yhat etc
  pivot_wider(names_from = "type", values_from = "value") %>%
  #spread(key="type",value="value") %>%
  #relable the lines with more description names, and order the factor for plotting:
  mutate(line=case_when(
           line=="line1" ~ "y ~ x",
           line=="line2" ~ "x ~ y",
           TRUE ~ "PCA"
         ),
         line = fct_relevel(line, "y ~ x", "x ~ y", "PCA")) %>% 
  ggplot() +
  geom_point(aes(x = x, y = y, color = line)) +
  geom_abline(aes(slope = slope, intercept = intercept, color = line)) +
  geom_segment(aes(x = x, y = y, xend = xhat, yend = yhat, color = line)) +
  facet_wrap(~ line, ncol = 3) +
  scale_color_manual(values = c("darkred", "blue", "darkorange")) +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = "none",
        strip.text = element_text(size = 16))
```


---

## So what do we do with the principal components?

__The point__: given a dataset with $p$ variables, we can find $k$ variables $(k << p)$ that account for most of the variation in the data

--

Note that the principal components are NOT easy to interpret - these are combinations of all variables

PCA is similar to MDS with these main differences:

1. MDS reduces a _distance_ matrix while PCA reduces a _data_ matrix

2. PCA has a principled way to choose $k$

3. Can visualize how the principal components are related to variables in data

---

## Working with PCA on Starbucks drinks

Use the `prcomp()` function (based on SVD) for PCA on __centered__ and __scaled__ data

```{r}
starbucks_pca <- prcomp(dplyr::select(starbucks, serv_size_m_l:caffeine_mg), #<<
                        center = TRUE, scale. = TRUE) #<<
summary(starbucks_pca)
```

---

## Computing Principal Components

Extract the matrix of principal components $\boldsymbol{Z} = XV$ (dimension of $\boldsymbol{Z}$ will match original data)

```{r}
starbucks_pc_matrix <- starbucks_pca$x
head(starbucks_pc_matrix)
```

Columns are uncorrelated, such that Var( $Z_1$ ) $>$ Var( $Z_2$ ) $> \dots >$ Var( $Z_p$ ) - can start with a scatterplot of $Z_1, Z_2$

---

## Starbucks drinks: PC1 and PC2

.pull-left[
```{r pca-plot, eval = FALSE}
starbucks <- starbucks %>%
  mutate(pc1 = starbucks_pc_matrix[,1], 
         pc2 = starbucks_pc_matrix[,2])
starbucks %>%
  ggplot(aes(x = pc1, y = pc2)) +
  geom_point(alpha = 0.5) +
  labs(x = "PC 1", y = "PC 2")
```

- __Look familiar?__

- Principal components are not interpretable, but we can add a __biplot__ with arrows showing the linear relationship between one variable and other variables

]

.pull-right[
```{r ref.label="pca-plot", echo = FALSE, fig.height=7}
```

]

---

## Making PCs interpretable with biplots ([`factoextra`](http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-visualization))

.pull-left[


```{r pca-biplot, eval = FALSE}
library(factoextra)
# Designate to only label the variables:
fviz_pca_biplot( #<<
  starbucks_pca, label = "var", #<<
  # Change the alpha for observations 
  # which is represented by ind
  alpha.ind = .25,
  # Modify the alpha for variables (var):
  alpha.var = .75,
  col.var = "darkblue")
```

- Arrow direction: "as the variable increases..."

- Arrow angles: correlation

  - 90 degrees means uncorrelated
  - $< 90$ means positively correlated
  - $> 90$ means negatively correlated
  
- Arrow length: strength of relationship with PCs

]
.pull-right[
```{r ref.label = "pca-biplot", echo = FALSE, fig.height=8}

```

]

---

## How many principal components to use?

#### Intuition: Additional principal components will add smaller and smaller variance

- Keep adding components until the added variance _drops off_

```{r}
summary(starbucks_pca)
```

---

## Create scree plot (aka "elbow plot") to choose

```{r scree-plot, eval = TRUE, fig.align='center', out.width="80%"}
fviz_eig(starbucks_pca, addlabels = TRUE) + #<<
  geom_hline(yintercept = 100 * (1 / ncol(starbucks_pca$x)), linetype = "dashed", color = "darkred")
```

- Number of dimensions on x-axis, proportion of variance on y-axis

- _Rule of thumb_: horizontal line at $1/p$ (__Why?__)

---

## Main Takeaways

#### It's really hard to visualize many dimensions at the same time

- Often, it makes a lot of sense to choose 2-4 of the "most important dimensions" and just plot those

#### PCA is a very common way to define "most important dimensions"

#### PCA provides the linear combinations of variables that capture the most variation in the data.

#### Common to plot the first two principal components in a scatterplot, and see if subjects cluster based on other variables (similar to MDS)

#### Especially useful to plot principal components with a biplot (e.g., with `factoextra`)

- Adds interpretability to the principal components, and helps you see relationships among the variables

--

+ __HW6 is Wednesday March 29th!__

+ __Graphics critique due April 7th!__

+ Review more code in Lecture 17 Demo! 


#### Next time: Spatial data

---

## BONUS: [__Singular Value Decomposition (SVD)__](https://en.wikipedia.org/wiki/Singular_value_decomposition)

$$
X = U D V^T
$$

- Matrices $U$ and $V$ contain the left and right (respectively) __singular vectors of scaled matrix $X$__

- $D$ is the diagonal matrix of the __singular values__

- SVD simplifies matrix-vector multiplication as __rotate, scale, and rotate again__


$V$ is called the __loading matrix__ for $X$ with $\phi_{j}$ as columns, 

  - $Z = X  V$ is the PC matrix

---

### BONUS: how to solve time-travel!

#### Eigenvalue decomposition (aka spectral decomposition)

- $V$ are the __eigenvectors__ of $X^TX$ (covariance matrix, $^T$ means _transpose_)

- $U$ are the __eigenvectors__ of $XX^T$

- The singular values (diagonal of $D$) are square roots of the __eigenvalues__ of $X^TX$ or $XX^T$

- Meaning that $Z = UD$

- `prcomp` function in `R` uses SVD for PCA

```{r, echo = FALSE, fig.align='center', out.width="45%"}
knitr::include_graphics("https://thumbs.gfycat.com/RealisticFragrantHerculesbeetle-size_restricted.gif")
```


---

## BONUS: Eigenvalues guide dimension reduction

We want to choose $p^* < p$ such that we are explaining variation in the data

Eigenvalues $\lambda_j$ for $j \in 1, \dots, p$ indicate __the variance explained by each component__

  - $\sum_j^p \lambda_j = p$, meaning $\lambda_j \geq 1$ indicates $\text{PC}j$ contains at least one variable's worth in variability
  
  - $\lambda_j / p$ equals proportion of variance explained by $\text{PC}j$
  
  - Arranged in descending order so that $\lambda_1$ is largest eigenvalue and corresponds to PC1
  
  - Can compute the cumulative proportion of variance explained (CVE) with $p^*$ components:
  
$$\text{CVE}_{p^*} = \frac{\sum_j^{p*} \lambda_j}{p}$$

Can use [__scree plot__](https://en.wikipedia.org/wiki/Scree_plot) to plot eigenvalues and guide choice for $p^* <p$ by looking for "elbow" (rapid to slow change)


